{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BlackCoffer Data Science intern.ipynb",
      "provenance": [],
      "mount_file_id": "1G4NmMYrz6MuFzz0DbfSyshqs49yJhERW",
      "authorship_tag": "ABX9TyPnAkLMMHkYmo2o7H0JYbLu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttd8fvVWddfo",
        "outputId": "323b49e8-11d5-41e4-b949-3dc83c8eb7a2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RioIKcwduG1",
        "outputId": "8c39b79f-c9fa-4ee4-a778-9bd0d52d2678"
      },
      "source": [
        "%cd /content/drive/MyDrive/Data Science\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1luUziVf3Yzfx_o_Q34NRaA0dRpJl7pRr/Data Science\n",
            " cik_list.xlsx\t\t       'Output Data Structure.xlsx'\n",
            " constraining_dictionary.xlsx  'Text Analysis.docx'\n",
            " Objective.docx\t\t        uncertainty_dictionary.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyaDlU1pd9nM",
        "outputId": "13de695b-119f-4de2-99d1-fc4eb550d04c"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import urllib.request\n",
        "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "QZI75oSNfzpm",
        "outputId": "89b824e4-869d-4ac5-bc8c-d2b657046fc7"
      },
      "source": [
        "org_file         = pd.read_excel(\"cik_list.xlsx\", header=None) #Reading the data from the Excel file into a DataFrame\n",
        "header           = org_file.iloc[0,:]                          #The header column names are being fetched \n",
        "org_file.columns = header                                      #The header column names are being assigned as the dataframe column names\n",
        "secfname_or      = org_file.copy()                             #Passing the dataframe values to secfname_or dataframe\n",
        "secfname_or      = secfname_or.iloc[1:,5]                      #Fetching the SECFNAME column from the dataframe\n",
        "file_rec         = org_file.iloc[1:,:]                         #1 is given instead of 0 because we do not want header as one of the record\n",
        "file_rec.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199803</td>\n",
              "      <td>1998-03-06 00:00:00</td>\n",
              "      <td>10-K405</td>\n",
              "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199805</td>\n",
              "      <td>1998-05-15 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199808</td>\n",
              "      <td>1998-08-13 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-12 00:00:00</td>\n",
              "      <td>10-K/A</td>\n",
              "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-16 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0         CIK  ...                                  SECFNAME\n",
              "1  0000003662  ...  edgar/data/3662/0000950170-98-000413.txt\n",
              "2  0000003662  ...  edgar/data/3662/0000950170-98-001001.txt\n",
              "3  0000003662  ...  edgar/data/3662/0000950172-98-000783.txt\n",
              "4  0000003662  ...  edgar/data/3662/0000950170-98-002145.txt\n",
              "5  0000003662  ...  edgar/data/3662/0000950172-98-001203.txt\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "Q_4kf1GUf3Tu",
        "outputId": "b9a75bd5-9971-41ba-ec7f-fc5f8244a997"
      },
      "source": [
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "#Add the below link at the prefix of all the values in SECFNAME column\n",
        "link = 'https://www.sec.gov/Archives/'\n",
        "file_rec['SECFNAME'] = link + file_rec['SECFNAME'].astype(str)\n",
        "\n",
        "#Now all the values in the SECFNAME column have become hyperlinks linking to \n",
        "#HTML pages\n",
        "file_rec.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199803</td>\n",
              "      <td>1998-03-06 00:00:00</td>\n",
              "      <td>10-K405</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199805</td>\n",
              "      <td>1998-05-15 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199808</td>\n",
              "      <td>1998-08-13 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-12 00:00:00</td>\n",
              "      <td>10-K/A</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-16 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "0         CIK  ...                                           SECFNAME\n",
              "1  0000003662  ...  https://www.sec.gov/Archives/edgar/data/3662/0...\n",
              "2  0000003662  ...  https://www.sec.gov/Archives/edgar/data/3662/0...\n",
              "3  0000003662  ...  https://www.sec.gov/Archives/edgar/data/3662/0...\n",
              "4  0000003662  ...  https://www.sec.gov/Archives/edgar/data/3662/0...\n",
              "5  0000003662  ...  https://www.sec.gov/Archives/edgar/data/3662/0...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oSK6qETf6Ou"
      },
      "source": [
        "new_columns = ['mda_positive_score', 'mda_negative_score', 'mda_polarity_score', 'mda_average_sentence_length', 'mda_percentage_of_complex_words', 'mda_fog_index',\n",
        "               'mda_complex_word_count', 'mda_word_count', 'mda_uncertainty_score', 'mda_constraining_score', 'mda_positive_word_proportion', 'mda_negative_word_proportion',\n",
        "               'mda_uncertainty_word_proportion', 'mda_constraining_word_proportion','qqdmr_positive_score','qqdmr_negative_score',\n",
        "               'qqdmr_polarity_score','qqdmr_average_sentence_length','qqdmr_percentage_of_complex_words', 'qqdmr_fog_index','qqdmr_complex_word_count',\n",
        "               'qqdmr_word_count', 'qqdmr_uncertainty_score', 'qqdmr_constraining_score', 'qqdmr_positive_word_proportion', 'qqdmr_negative_word_proportion',\n",
        "               'qqdmr_uncertainty_word_proportion', 'qqdmr_constraining_word_proportion', 'rf_positive_score', 'rf_negative_score',\n",
        "               'rf_polarity_score', 'rf_average_sentence_length', 'rf_percentage_of_complex_words', 'rf_fog_index', 'rf_complex_word_count',\n",
        "               'rf_word_count', 'rf_uncertainty_score', 'rf_constraining_score', 'rf_positive_word_proportion', 'rf_negative_word_proportion',\n",
        "               'rf_uncertainty_word_proportion', 'rf_constraining_word_proportion', 'constraining_words_whole_report']\n",
        "\n",
        "file_rec = pd.concat([file_rec,pd.DataFrame(columns=new_columns)]) #Appends the above columns to the existing dataframe file_rec\n",
        "file_rec = file_rec.reset_index()                                  #Resets the row index of the dataframe starting from 0\n",
        "del file_rec['index']                                              #Deleting the redundant index column from the dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "QKotRB4xf-A2",
        "outputId": "5cfafed0-55b2-4190-d6c9-056a88eacec6"
      },
      "source": [
        "file_rec.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "      <th>mda_positive_score</th>\n",
              "      <th>mda_negative_score</th>\n",
              "      <th>mda_polarity_score</th>\n",
              "      <th>mda_average_sentence_length</th>\n",
              "      <th>mda_percentage_of_complex_words</th>\n",
              "      <th>mda_fog_index</th>\n",
              "      <th>mda_complex_word_count</th>\n",
              "      <th>mda_word_count</th>\n",
              "      <th>mda_uncertainty_score</th>\n",
              "      <th>mda_constraining_score</th>\n",
              "      <th>mda_positive_word_proportion</th>\n",
              "      <th>mda_negative_word_proportion</th>\n",
              "      <th>mda_uncertainty_word_proportion</th>\n",
              "      <th>mda_constraining_word_proportion</th>\n",
              "      <th>qqdmr_positive_score</th>\n",
              "      <th>qqdmr_negative_score</th>\n",
              "      <th>qqdmr_polarity_score</th>\n",
              "      <th>qqdmr_average_sentence_length</th>\n",
              "      <th>qqdmr_percentage_of_complex_words</th>\n",
              "      <th>qqdmr_fog_index</th>\n",
              "      <th>qqdmr_complex_word_count</th>\n",
              "      <th>qqdmr_word_count</th>\n",
              "      <th>qqdmr_uncertainty_score</th>\n",
              "      <th>qqdmr_constraining_score</th>\n",
              "      <th>qqdmr_positive_word_proportion</th>\n",
              "      <th>qqdmr_negative_word_proportion</th>\n",
              "      <th>qqdmr_uncertainty_word_proportion</th>\n",
              "      <th>qqdmr_constraining_word_proportion</th>\n",
              "      <th>rf_positive_score</th>\n",
              "      <th>rf_negative_score</th>\n",
              "      <th>rf_polarity_score</th>\n",
              "      <th>rf_average_sentence_length</th>\n",
              "      <th>rf_percentage_of_complex_words</th>\n",
              "      <th>rf_fog_index</th>\n",
              "      <th>rf_complex_word_count</th>\n",
              "      <th>rf_word_count</th>\n",
              "      <th>rf_uncertainty_score</th>\n",
              "      <th>rf_constraining_score</th>\n",
              "      <th>rf_positive_word_proportion</th>\n",
              "      <th>rf_negative_word_proportion</th>\n",
              "      <th>rf_uncertainty_word_proportion</th>\n",
              "      <th>rf_constraining_word_proportion</th>\n",
              "      <th>constraining_words_whole_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199803</td>\n",
              "      <td>1998-03-06 00:00:00</td>\n",
              "      <td>10-K405</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199805</td>\n",
              "      <td>1998-05-15 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199808</td>\n",
              "      <td>1998-08-13 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          CIK  ... constraining_words_whole_report\n",
              "0  0000003662  ...                             NaN\n",
              "1  0000003662  ...                             NaN\n",
              "2  0000003662  ...                             NaN\n",
              "\n",
              "[3 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeJa5DhmgGu0"
      },
      "source": [
        "#Fetching the Stop words list\n",
        "with open('/content/StopWords_GenericLong.txt') as f:\n",
        "   stop_words = list(f) \n",
        "   stop_words = [s.rstrip() for s in stop_words]\n",
        "   stop_words = [x.lower() for x in stop_words]\n",
        "\n",
        "#Fetching the list of Positive and Negative words\n",
        "positive = pd.read_excel(\"/content/LoughranMcDonald_SentimentWordLists_2018.xlsx\", \"Positive\", header=None)\n",
        "negative = pd.read_excel(\"/content/LoughranMcDonald_SentimentWordLists_2018.xlsx\", \"Negative\", header=None)\n",
        "\n",
        "#Fetching all the positive and Negative words into two separate lists\n",
        "positive = positive[0]\n",
        "negative = negative[0]\n",
        "\n",
        "#Keeping all the positive and Negative words which are not present in stop words list\n",
        "positive_cleaned = list(set(positive)-set(stop_words)) \n",
        "negative_cleaned = list(set(negative)-set(stop_words)) \n",
        "\n",
        "#Converting the Positive and Negative words to Lowercase\n",
        "positive_cleaned = [x.lower() for x in positive_cleaned]\n",
        "negative_cleaned = [x.lower() for x in negative_cleaned]\n",
        "\n",
        "#Fetching the uncertainity and constarining words from the Excel sheet\n",
        "uncertainity_dict = pd.read_excel(\"uncertainty_dictionary.xlsx\", header=None)\n",
        "constraining_dict     = pd.read_excel(\"constraining_dictionary.xlsx\", header=None)\n",
        "\n",
        "#Passing all the above obtained words into separate lists and converting it to Lowercase\n",
        "uncertainity_words = uncertainity_dict[0].tolist()\n",
        "uncertainity_words = [x.lower() for x in uncertainity_words]\n",
        "\n",
        "constraining_words = constraining_dict[0].tolist()\n",
        "constraining_words = [x.lower() for x in constraining_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy2fNLuN_hwU"
      },
      "source": [
        "###################################################################################################################\n",
        "#                                FUNCTION FOR FETCHING THE SECTION's DATA                                          |\n",
        "###################################################################################################################\n",
        "\n",
        "def fetch_sections_data(data):\n",
        "\n",
        "################################################################################\n",
        "#       Fetching the \"MANAGEMENT'S DISCUSSION AND ANALYSIS\" section             |\n",
        "################################################################################\n",
        "\n",
        "  topic1 = \"MANAGEMENT'S DISCUSSION AND ANALYSIS\"\n",
        "  topic1_start = [m.start() for m in re.finditer(topic1, data)]\n",
        "\n",
        "  if (len(topic1_start) == 0):\n",
        "    topic1 = \"MANAGEMENTS DISCUSSION AND ANALYSIS\"\n",
        "    topic1_start = [m.start() for m in re.finditer(topic1, data)]\n",
        "\n",
        "      \n",
        "  if (len(topic1_start) != 0):\n",
        "    #topic_1_data has the data from the point where topic1 begins\n",
        "    topic_1_data = data[topic1_start[0]:]\n",
        "    \n",
        "    item = \"ITEM\"\n",
        "    topic1_end = [m.start() for m in re.finditer(item, topic_1_data)]\n",
        "\n",
        "    if (len(topic1_end) == 0):\n",
        "      topic_1_data = topic_1_data[0:]\n",
        "    else:  \n",
        "      topic_1_data = topic_1_data[0:topic1_end[0]-1]\n",
        "    #topic_1_data ends at the point where topic1 ends\n",
        "  if (len(topic1_start) == 0):\n",
        "    topic_1_data = 0  \n",
        "\n",
        "#####################################################################################################\n",
        "#       Fetching the \"QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\" section            |\n",
        "#####################################################################################################\n",
        "\n",
        "  topic2 = \"QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\"\n",
        "  topic2_start = [m.start() for m in re.finditer(topic2, data)]\n",
        "  \n",
        "  if (len(topic2_start) != 0):\n",
        "\n",
        "    topic_2_data = data[topic2_start[0]:]\n",
        "\n",
        "    item = \"ITEM\"\n",
        "    topic2_end = [m.start() for m in re.finditer(item, topic_2_data)]\n",
        "    if (len(topic2_end) == 0):\n",
        "      topic_2_data = topic_2_data[0:]\n",
        "    else:  \n",
        "      topic_2_data = topic_2_data[0:topic2_end[0]-1]     \n",
        "  \n",
        "  if (len(topic2_start) == 0):\n",
        "    topic_2_data = 0 \n",
        "\n",
        "  #SECTION 3 - Risk Factors\n",
        "  topic3 = \"RISK FACTORS\"\n",
        "  topic3_start = [m.start() for m in re.finditer(topic3, data)]\n",
        "\n",
        "  if (len(topic3_start) != 0):\n",
        "\n",
        "    topic_3_data = data[topic3_start[0]:]\n",
        "    \n",
        "    item = \"ITEM\"\n",
        "    topic3_end = [m.start() for m in re.finditer(item, topic_3_data)]\n",
        "    \n",
        "    if (len(topic3_end) == 0):\n",
        "      topic_3_data = topic_3_data[0:]\n",
        "    else:  \n",
        "      topic_3_data = topic_3_data[0:topic3_end[0]-1]    \n",
        "\n",
        "  if (len(topic3_start) == 0):\n",
        "    topic_3_data = 0         \n",
        "\n",
        "  return topic_1_data, topic_2_data, topic_3_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYlCpUaTh42l"
      },
      "source": [
        "###################################################################################################################\n",
        "#                      FUNCTION FOR CALCULATING CONSTRAINING WORDS FOR THE WHOLE REPORT                            |\n",
        "###################################################################################################################\n",
        "\n",
        "def constr_whole_report(x):\n",
        "\n",
        "  x      = x.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations from the string before tokenizing\n",
        "  tokens = nltk.word_tokenize(x)                                #Tokenizing the string into a list of words\n",
        "  tokens = [x.lower() for x in tokens]                          #Converting all the words in thee list to lowercase\n",
        "\n",
        "  constr_whole_words = 0\n",
        "  for word in tokens:                                         \n",
        "\n",
        "    if word in constraining_words:                              #If word is present in constraining word list we increment constr_words by 1                \n",
        "      constr_whole_words += 1     \n",
        "      \n",
        "  return constr_whole_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWkTSJzb_0Mw"
      },
      "source": [
        "\n",
        "###################################################################################################################\n",
        "#                  DATA CLEANING - REMOVING THE WORDS PRESENT IN STOP WORDS LIST                                   |\n",
        "###################################################################################################################\n",
        "\n",
        "def data_cleaning(x1, x2, x3):\n",
        "\n",
        "  if (x1 != 0):\n",
        "    x1 = nltk.word_tokenize(x1)                                     \n",
        "    x1 = [word for word in x1 if word.lower() not in stop_words]  #Gets all the words which are not present in the stop words list\n",
        "    x1 = ' '.join(x1)                                             #Combines all the cleaned data into a string of words                                        \n",
        "\n",
        "  if (x2 != 0):\n",
        "    x2 = nltk.word_tokenize(x2)                                   #Tokenizes words in the list\n",
        "    x2 = [word for word in x2 if word.lower() not in stop_words]  #Gets all the words which are not present in the stop words list\n",
        "    x2 = ' '.join(x2)                                             #Combines all the cleaned data into a string of words                          \n",
        "\n",
        "\n",
        "  if (x3 != 0):\n",
        "    x3 = nltk.word_tokenize(x3)                                   #Tokenizes words in the list\n",
        "    x3 = [word for word in x3 if word.lower() not in stop_words]  #Gets all the words which are not present in the stop words list\n",
        "    x3 = ' '.join(x3)                                             #Combines all the cleaned data into a string of words\n",
        "\n",
        "  return x1, x2, x3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQi5TEA8AGd4"
      },
      "source": [
        "###################################################################################################################\n",
        "#                 FUNCTION FOR CALCULATING POSITIVE SCORE, NEGATIVE SCORE AND POLARITY SCORE                       |\n",
        "###################################################################################################################\n",
        "\n",
        "def pos_neg_pol(x):\n",
        "\n",
        "  x      = x.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations from the string before tokenizing \n",
        "  tokens = nltk.word_tokenize(x)                                #Tokenizing the string into a list of words\n",
        "  tokens = [x.lower() for x in tokens]                          #Converting all the words in the list to lower case\n",
        "\n",
        "  pos_words = neg_words = 0\n",
        "\n",
        "  for word in tokens:\n",
        "    if word in positive_cleaned:\n",
        "      pos_words += 1\n",
        "    if word in negative_cleaned:\n",
        "      neg_words -= 1 \n",
        "\n",
        "  pos_len = pos_words\n",
        "  neg_len = neg_words * -1\n",
        "\n",
        "  pol_score = (pos_len - neg_len)/((pos_len + neg_len) + 0.000001)\n",
        "\n",
        "  return pos_len, neg_len, pol_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52cK1oWB0vl"
      },
      "source": [
        "\n",
        "###################################################################################################################\n",
        "#                          FUNCTION FOR CALCULATING AVERAGE SENTENCE LENGTH IN SECTION'S DATA                      |\n",
        "###################################################################################################################\n",
        "\n",
        "def avg_sent_len(x):\n",
        "\n",
        "  no_sent = len(sent_tokenize(x))                               #Calculating the number of sentences in the whole data\n",
        "  x      = x.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations from the string before tokenizing\n",
        "  tokens = nltk.word_tokenize(x)                                #Tokenizing the string into a list of words\n",
        "  no_words = len(tokens)                                        #Calculating the number of words in the ddata\n",
        "\n",
        "  if (no_sent != 0):\n",
        "    avg_sent_len = round(no_words/no_sent)                      #Calculating the Average Sentence length\n",
        "  else:\n",
        "    avg_sent_len = 0\n",
        "\n",
        "  return avg_sent_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzbRMBuSB6Rg"
      },
      "source": [
        "\n",
        "###################################################################################################################\n",
        "#                    FUNCTION FOR CALCULATING WORD_COUNT, COMPLEX WORD COUNT AND PERC OF COMPLEX WORDS             |\n",
        "###################################################################################################################\n",
        "\n",
        "def complex_word_count(word, complex_count):\n",
        "\n",
        "    word   = word.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations from the string before tokenizing\n",
        "    tokens = nltk.word_tokenize(word)                                #Tokenizing using nltk.word_tokenize\n",
        "    no_words = len(tokens)                                           #Finding out the number of words\n",
        "    \n",
        "    complex_count = 0                                                #Initializing the complex_count variable to 0\n",
        "    for word in tokens:\n",
        "\n",
        "      word = word.lower()                                            #Converting all the words in the list to Lowercase\n",
        "\n",
        "      vowels = \"aeiou\"    \n",
        "\n",
        "      if (word.endswith((\"es\", \"ed\"))):                              #We are ignoring the words ending with es or ed in our calculation\n",
        "        count = 0\n",
        "\n",
        "      else:\n",
        "        count = 0\n",
        "        \n",
        "        for c in word:\n",
        "          if (c in vowels):\n",
        "            count = count + 1                                        #Counting the number of Vowels in each word\n",
        "\n",
        "        if (count > 2):                                              #If number of vowels > 2 then we are incrementing complex_count variable by 1\n",
        "          complex_count = complex_count + 1 \n",
        "\n",
        "    perc_complex_words = complex_count/no_words                      #Calculating the percentage of complex words\n",
        "\n",
        "    return no_words, complex_count, perc_complex_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7DzW61SCBwt"
      },
      "source": [
        "\n",
        "###################################################################################################################\n",
        "#                        FUNCTION FOR CALCULATING UNCERTAINITY AND CONSTRAINING SCORE                              |\n",
        "###################################################################################################################\n",
        "\n",
        "def uncer_constr_calc(x):\n",
        "  \n",
        "  x      = x.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations from the string before tokenizing\n",
        "  tokens = nltk.word_tokenize(x)                                #Tokenizing the string into a list of words\n",
        "  tokens = [x.lower() for x in tokens]                          #Converting all the words in thee list to lowercase\n",
        "\n",
        "  uncer_words = constr_words = 0\n",
        "\n",
        "  for word in tokens:                                         \n",
        "    if word in uncertainity_words:                              #If word is present in uncertainity word list we increment uncer_words by 1\n",
        "      uncer_words  += 1\n",
        "    if word in constraining_words:                              #If word is present in constraining word list we increment constr_words by 1                \n",
        "      constr_words += 1 \n",
        "\n",
        "  uncer_count  = uncer_words\n",
        "  constr_count = constr_words\n",
        "\n",
        "  return uncer_count, constr_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEGnKcrigQJY",
        "outputId": "afe1a19e-afce-40ea-8e54-094728ec5e87"
      },
      "source": [
        "###################################################################################################################\n",
        "#                                      MAIN PROGRAM OPERATIONS                                                     |\n",
        "###################################################################################################################\n",
        "\n",
        "for i in range(len(file_rec)):                    #Looping through the file_rec dataframe\n",
        "  \n",
        "  url = file_rec.iloc[i,5]   \n",
        "  try:                     #Fetching the URL from the dataframe\n",
        "    data = urllib.request.urlopen(url).readlines()  #Reading the contents from the URL link page\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "  #Data cleaning to optimize the speed and efficiency of operation\n",
        "  data = [str(s, 'utf-8') for s in data]          #Decoding the variable to remove b(bytes object) at the beginning\n",
        "  data = [s.strip('\\n') for s in data]            #Removing the new line character \\n at the end of every line\n",
        "  data = list(filter(None, data))                 #Removing the lines which are blank\n",
        "  #After doing the above operations the number of iterations required now would be less\n",
        "\n",
        "  #Converting the list of strings into a single string to increase speed of operation instead of looping again\n",
        "  data = \" \".join(data)\n",
        "\n",
        "  #Removing the Tables from the data\n",
        "  data = re.sub(\"(?is)<table[^>]*>(.*?)<\\/table>\", \"\", data)\n",
        "\n",
        "  #REGEX operations to remove the HTML content from our data and get a clean text data\n",
        "  html_regex = re.compile(r'<.*?>')\n",
        "  data = re.sub(html_regex,'',data)\n",
        "  data = data.replace('&nbsp;','')\n",
        "  input_data = re.sub(r'&#\\d+;', '', data)   \n",
        "\n",
        "  #Calculating the Constraining words for the whole report\n",
        "  constr_whole_count = constr_whole_report(input_data)\n",
        "  file_rec.loc[i, 'constraining_words_whole_report'] = constr_whole_count\n",
        "\n",
        "  #Fetching the sections data using the fetch_sections_data()\n",
        "  topic1_data, topic2_data, topic3_data = fetch_sections_data(input_data)\n",
        "\n",
        "  #Removing the Stop words, punctuations in our data\n",
        "  topic1_data, topic2_data, topic3_data = data_cleaning(topic1_data, topic2_data, topic3_data)\n",
        "\n",
        "################################################################################\n",
        "#   Calculating the values for \"MANAGEMENT'S DISCUSSION AND ANALYSIS\" section   |\n",
        "################################################################################\n",
        "\n",
        "  if (topic1_data != 0):\n",
        "    \n",
        "    #Calculating the positive score, negative score and Polarity score\n",
        "    pos_score1, neg_score1, pol_score1 = pos_neg_pol(topic1_data)\n",
        "\n",
        "    #Passing the above values to our corresponding dataframe records\n",
        "    file_rec.loc[i, 'mda_positive_score'] = pos_score1\n",
        "    file_rec.loc[i, 'mda_negative_score'] = neg_score1 \n",
        "    file_rec.loc[i, 'mda_polarity_score'] = pol_score1\n",
        "\n",
        "    #Calculating the average sentence length for our section of data\n",
        "    avg_sent_len_val1 = avg_sent_len(topic1_data)\n",
        "    file_rec.loc[i, 'mda_average_sentence_length'] = avg_sent_len_val1\n",
        "\n",
        "    #Calculating the word count, complex word count and percentage of complex words\n",
        "    complex_count = 0\n",
        "    word_count1, complex_count1, perc_complex_words1 = complex_word_count(topic1_data, complex_count)\n",
        "    file_rec.loc[i, 'mda_word_count']                  = word_count1\n",
        "    file_rec.loc[i, 'mda_complex_word_count']          = complex_count1\n",
        "    file_rec.loc[i, 'mda_percentage_of_complex_words'] = perc_complex_words1\n",
        "\n",
        "    #Calculating the Fog index value \n",
        "    fog_index1 = 0.4 * (avg_sent_len_val1 + perc_complex_words1)\n",
        "    file_rec.loc[i, 'mda_fog_index'] = fog_index1\n",
        "\n",
        "    #Calculating the uncertainity score and constraining score\n",
        "    uncer_score1, constr_score1 = uncer_constr_calc(topic1_data)\n",
        "    perc_uncer1  = uncer_score1/word_count1\n",
        "    perc_constr1 = constr_score1/word_count1\n",
        "    file_rec.loc[i, 'mda_uncertainty_score']            = uncer_score1\n",
        "    file_rec.loc[i, 'mda_constraining_score']           = constr_score1\n",
        "\n",
        "    #Calculating the Uncertainity and Constraining word proportion\n",
        "    file_rec.loc[i, 'mda_uncertainty_word_proportion']  = perc_uncer1\n",
        "    file_rec.loc[i, 'mda_constraining_word_proportion'] = perc_constr1\n",
        "    \n",
        "    #Calculating the Positive and Negative word proportion\n",
        "    perc_positive1 = pos_score1/word_count1\n",
        "    perc_negative1 = neg_score1/word_count1\n",
        "    file_rec.loc[i, 'mda_positive_word_proportion']  = perc_positive1\n",
        "    file_rec.loc[i, 'mda_negative_word_proportion']  = perc_negative1\n",
        "\n",
        "  else:\n",
        "    #If the section 1 data is not available, all the below values should be 0\n",
        "    file_rec.loc[i, 6:21] = 0\n",
        "\n",
        "#####################################################################################################\n",
        "#  Calculating the values for \"QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\" section   |\n",
        "#####################################################################################################\n",
        "\n",
        "  if (topic2_data != 0):  \n",
        "    \n",
        "    #Calculating the positive score, negative score and Polarity score\n",
        "    pos_score2, neg_score2, pol_score2 = pos_neg_pol(topic2_data)\n",
        "    #Passing the above values to our corresponding dataframe records\n",
        "    file_rec.loc[i, 'qqdmr_positive_score'] = pos_score2\n",
        "    file_rec.loc[i, 'qqdmr_negative_score'] = neg_score2 \n",
        "    file_rec.loc[i, 'qqdmr_polarity_score'] = pol_score2\n",
        "\n",
        "    #Calculating the average sentence length for our section of data\n",
        "    avg_sent_len_val2 = avg_sent_len(topic2_data)\n",
        "    file_rec.loc[i, 'qqdmr_average_sentence_length'] = avg_sent_len_val2\n",
        "\n",
        "    #Calculating the word count, complex word count and percentage of complex words\n",
        "    complex_count = 0\n",
        "    word_count2, complex_count2, perc_complex_words2 = complex_word_count(topic2_data, complex_count)\n",
        "    file_rec.loc[i, 'qqdmr_word_count']                  = word_count2\n",
        "    file_rec.loc[i, 'qqdmr_complex_word_count']          = complex_count2\n",
        "    file_rec.loc[i, 'qqdmr_percentage_of_complex_words'] = perc_complex_words2\n",
        "\n",
        "    #Calculating the Fog index value \n",
        "    fog_index2 = 0.4 * (avg_sent_len_val2 + perc_complex_words2)\n",
        "    file_rec.loc[i, 'qqdmr_fog_index'] = fog_index2\n",
        "\n",
        "    #Calculating the uncertainity score and constraining score\n",
        "    uncer_score2, constr_score2 = uncer_constr_calc(topic2_data)\n",
        "    perc_uncer2  = uncer_score2/word_count2\n",
        "    perc_constr2 = constr_score2/word_count2\n",
        "    file_rec.loc[i, 'qqdmr_uncertainty_score']            = uncer_score2\n",
        "    file_rec.loc[i, 'qqdmr_constraining_score']           = constr_score2\n",
        "\n",
        "    #Calculating the Uncertainity and Constraining word proportion\n",
        "    file_rec.loc[i, 'qqdmr_uncertainty_word_proportion']  = perc_uncer2\n",
        "    file_rec.loc[i, 'qqdmr_constraining_word_proportion'] = perc_constr2\n",
        "    \n",
        "    #Calculating the Positive and Negative word proportion\n",
        "    perc_positive2 = pos_score2/word_count2\n",
        "    perc_negative2 = neg_score2/word_count2\n",
        "    file_rec.loc[i, 'qqdmr_positive_word_proportion']  = perc_positive2\n",
        "    file_rec.loc[i, 'qqdmr_negative_word_proportion']  = perc_negative2\n",
        "\n",
        "  else:\n",
        "    #If the section 2 data is not available, all the below values should be 0\n",
        "    file_rec.loc[i, 20:33] = 0\n",
        "\n",
        "################################################################################\n",
        "#            Calculating the values for \"RISK FACTORS\" section                  |\n",
        "################################################################################\n",
        "\n",
        "  if (topic3_data != 0):  \n",
        "    \n",
        "    #Calculating the positive score, negative score and Polarity score\n",
        "    pos_score3, neg_score3, pol_score3 = pos_neg_pol(topic3_data)\n",
        "    #Passing the above values to our corresponding dataframe records\n",
        "    file_rec.loc[i, 'rf_positive_score'] = pos_score3\n",
        "    file_rec.loc[i, 'rf_negative_score'] = neg_score3 \n",
        "    file_rec.loc[i, 'rf_polarity_score'] = pol_score3\n",
        "\n",
        "    #Calculating the average sentence length for our section of data\n",
        "    avg_sent_len_val3 = avg_sent_len(topic3_data)\n",
        "    file_rec.loc[i, 'rf_average_sentence_length'] = avg_sent_len_val3\n",
        "\n",
        "    #Calculating the word count, complex word count and percentage of complex words\n",
        "    complex_count = 0\n",
        "    word_count3, complex_count3, perc_complex_words3 = complex_word_count(topic3_data, complex_count)\n",
        "    file_rec.loc[i, 'rf_word_count']                  = word_count3\n",
        "    file_rec.loc[i, 'rf_complex_word_count']          = complex_count3\n",
        "    file_rec.loc[i, 'rf_percentage_of_complex_words'] = perc_complex_words3\n",
        "\n",
        "    #Calculating the Fog index value \n",
        "    fog_index3 = 0.4 * (avg_sent_len_val3 + perc_complex_words3)\n",
        "    file_rec.loc[i, 'rf_fog_index'] = fog_index3\n",
        "\n",
        "    #Calculating the uncertainity score and constraining score\n",
        "    uncer_score3, constr_score3 = uncer_constr_calc(topic3_data)\n",
        "    perc_uncer3  = uncer_score3/word_count3\n",
        "    perc_constr3 = constr_score3/word_count3\n",
        "    file_rec.loc[i, 'rf_uncertainty_score']            = uncer_score3\n",
        "    file_rec.loc[i, 'rf_constraining_score']           = constr_score3\n",
        "    #Calculating the Uncertainity and Constraining word proportion\n",
        "    file_rec.loc[i, 'rf_uncertainty_word_proportion']  = perc_uncer3\n",
        "    file_rec.loc[i, 'rf_constraining_word_proportion'] = perc_constr3\n",
        "    \n",
        "    #Calculating the Positive and Negative word proportion\n",
        "    perc_positive3 = pos_score3/word_count3\n",
        "    perc_negative3 = neg_score3/word_count3\n",
        "    file_rec.loc[i, 'rf_positive_word_proportion']  = perc_positive3\n",
        "    file_rec.loc[i, 'rf_negative_word_proportion']  = perc_negative3\n",
        "\n",
        "  else:\n",
        "    #If the section 1 data is not available, all the below values should be 0\n",
        "    file_rec.loc[i, 34:47] = 0\n",
        "\n",
        "  #Clearing all the variables\n",
        "  pos_score1 = neg_score1 = pol_score1 = avg_sent_len_val1 = word_count1 = complex_count1 = perc_complex_words1 = fog_index1 = uncer_score1 = constr_score1 = perc_uncer1 = perc_constr1 = perc_positive1 = perc_negative1 = 0\n",
        "  pos_score2 = neg_score2 = pol_score2 = avg_sent_len_val2 = word_count2 = complex_count2 = perc_complex_words2 = fog_index2 = uncer_score2 = constr_score2 = perc_uncer2 = perc_constr2 = perc_positive2 = perc_negative2 = 0\n",
        "  pos_score3 = neg_score3 = pol_score3 = avg_sent_len_val3 = word_count3 = complex_count3 = perc_complex_words3 = fog_index3 = uncer_score3 = constr_score3 = perc_uncer3 = perc_constr3 = perc_positive3 = perc_negative3 = 0\n",
        "  constr_whole_count = 0\n",
        "\n",
        "print(\"All operations on the dataframe have finished successfully\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:186: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: FutureWarning: Slicing a positional slice with .loc is not supported, and will raise TypeError in a future version.  Use .loc with labels or .iloc with positions instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All operations on the dataframe have finished successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "c_SWPhBuMGXG",
        "outputId": "93716bf3-0895-44cc-b16c-62b7d8ef6102"
      },
      "source": [
        "#Removing the pre-fix http link at the beginning to keep values same as original file\n",
        "link = 'https://www.sec.gov/Archives/'\n",
        "secfname_or = secfname_or.reset_index(drop=True)\n",
        "file_rec['SECFNAME'] = secfname_or \n",
        "\n",
        "#Final output is stored in file_rec dataframe\n",
        "file_rec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CIK</th>\n",
              "      <th>CONAME</th>\n",
              "      <th>FYRMO</th>\n",
              "      <th>FDATE</th>\n",
              "      <th>FORM</th>\n",
              "      <th>SECFNAME</th>\n",
              "      <th>mda_positive_score</th>\n",
              "      <th>mda_negative_score</th>\n",
              "      <th>mda_polarity_score</th>\n",
              "      <th>mda_average_sentence_length</th>\n",
              "      <th>mda_percentage_of_complex_words</th>\n",
              "      <th>mda_fog_index</th>\n",
              "      <th>mda_complex_word_count</th>\n",
              "      <th>mda_word_count</th>\n",
              "      <th>mda_uncertainty_score</th>\n",
              "      <th>mda_constraining_score</th>\n",
              "      <th>mda_positive_word_proportion</th>\n",
              "      <th>mda_negative_word_proportion</th>\n",
              "      <th>mda_uncertainty_word_proportion</th>\n",
              "      <th>mda_constraining_word_proportion</th>\n",
              "      <th>qqdmr_positive_score</th>\n",
              "      <th>qqdmr_negative_score</th>\n",
              "      <th>qqdmr_polarity_score</th>\n",
              "      <th>qqdmr_average_sentence_length</th>\n",
              "      <th>qqdmr_percentage_of_complex_words</th>\n",
              "      <th>qqdmr_fog_index</th>\n",
              "      <th>qqdmr_complex_word_count</th>\n",
              "      <th>qqdmr_word_count</th>\n",
              "      <th>qqdmr_uncertainty_score</th>\n",
              "      <th>qqdmr_constraining_score</th>\n",
              "      <th>qqdmr_positive_word_proportion</th>\n",
              "      <th>qqdmr_negative_word_proportion</th>\n",
              "      <th>qqdmr_uncertainty_word_proportion</th>\n",
              "      <th>qqdmr_constraining_word_proportion</th>\n",
              "      <th>rf_positive_score</th>\n",
              "      <th>rf_negative_score</th>\n",
              "      <th>rf_polarity_score</th>\n",
              "      <th>rf_average_sentence_length</th>\n",
              "      <th>rf_percentage_of_complex_words</th>\n",
              "      <th>rf_fog_index</th>\n",
              "      <th>rf_complex_word_count</th>\n",
              "      <th>rf_word_count</th>\n",
              "      <th>rf_uncertainty_score</th>\n",
              "      <th>rf_constraining_score</th>\n",
              "      <th>rf_positive_word_proportion</th>\n",
              "      <th>rf_negative_word_proportion</th>\n",
              "      <th>rf_uncertainty_word_proportion</th>\n",
              "      <th>rf_constraining_word_proportion</th>\n",
              "      <th>constraining_words_whole_report</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199803</td>\n",
              "      <td>1998-03-06 00:00:00</td>\n",
              "      <td>10-K405</td>\n",
              "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
              "      <td>20</td>\n",
              "      <td>71</td>\n",
              "      <td>-0.56044</td>\n",
              "      <td>21</td>\n",
              "      <td>0.379893</td>\n",
              "      <td>8.55196</td>\n",
              "      <td>854</td>\n",
              "      <td>2248</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0088968</td>\n",
              "      <td>0.0315836</td>\n",
              "      <td>0.0133452</td>\n",
              "      <td>0.0044484</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199805</td>\n",
              "      <td>1998-05-15 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199808</td>\n",
              "      <td>1998-08-13 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-12 00:00:00</td>\n",
              "      <td>10-K/A</td>\n",
              "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
              "      <td>40</td>\n",
              "      <td>131</td>\n",
              "      <td>-0.532164</td>\n",
              "      <td>19</td>\n",
              "      <td>0.412589</td>\n",
              "      <td>7.76504</td>\n",
              "      <td>1678</td>\n",
              "      <td>4067</td>\n",
              "      <td>75</td>\n",
              "      <td>46</td>\n",
              "      <td>0.00983526</td>\n",
              "      <td>0.0322105</td>\n",
              "      <td>0.0184411</td>\n",
              "      <td>0.0113105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000003662</td>\n",
              "      <td>SUNBEAM CORP/FL/</td>\n",
              "      <td>199811</td>\n",
              "      <td>1998-11-16 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0000012239</td>\n",
              "      <td>SPHERIX INC</td>\n",
              "      <td>200704</td>\n",
              "      <td>2007-04-02 00:00:00</td>\n",
              "      <td>10-K</td>\n",
              "      <td>edgar/data/12239/0001104659-07-024804.txt</td>\n",
              "      <td>109</td>\n",
              "      <td>124</td>\n",
              "      <td>-0.0643777</td>\n",
              "      <td>13</td>\n",
              "      <td>0.388878</td>\n",
              "      <td>5.35555</td>\n",
              "      <td>3867</td>\n",
              "      <td>9944</td>\n",
              "      <td>89</td>\n",
              "      <td>84</td>\n",
              "      <td>0.0109614</td>\n",
              "      <td>0.0124698</td>\n",
              "      <td>0.00895012</td>\n",
              "      <td>0.0084473</td>\n",
              "      <td>85</td>\n",
              "      <td>87</td>\n",
              "      <td>-0.0116279</td>\n",
              "      <td>13</td>\n",
              "      <td>0.38697</td>\n",
              "      <td>5.35479</td>\n",
              "      <td>2946</td>\n",
              "      <td>7613</td>\n",
              "      <td>50</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0111651</td>\n",
              "      <td>0.0114278</td>\n",
              "      <td>0.00656771</td>\n",
              "      <td>0.0077499</td>\n",
              "      <td>134</td>\n",
              "      <td>185</td>\n",
              "      <td>-0.159875</td>\n",
              "      <td>13</td>\n",
              "      <td>0.393503</td>\n",
              "      <td>5.3574</td>\n",
              "      <td>4676</td>\n",
              "      <td>11883</td>\n",
              "      <td>117</td>\n",
              "      <td>107</td>\n",
              "      <td>0.0112766</td>\n",
              "      <td>0.0155685</td>\n",
              "      <td>0.009846</td>\n",
              "      <td>0.00900446</td>\n",
              "      <td>130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0000012239</td>\n",
              "      <td>SPHERIX INC</td>\n",
              "      <td>200705</td>\n",
              "      <td>2007-05-16 00:00:00</td>\n",
              "      <td>NT 10-Q</td>\n",
              "      <td>edgar/data/12239/0001104659-07-040463.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>0000012239</td>\n",
              "      <td>SPHERIX INC</td>\n",
              "      <td>200705</td>\n",
              "      <td>2007-05-18 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/12239/0001104659-07-041441.txt</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>0000012239</td>\n",
              "      <td>SPHERIX INC</td>\n",
              "      <td>200705</td>\n",
              "      <td>2007-05-23 00:00:00</td>\n",
              "      <td>10-K/A</td>\n",
              "      <td>edgar/data/12239/0001104659-07-042333.txt</td>\n",
              "      <td>109</td>\n",
              "      <td>124</td>\n",
              "      <td>-0.0643777</td>\n",
              "      <td>13</td>\n",
              "      <td>0.389317</td>\n",
              "      <td>5.35573</td>\n",
              "      <td>3841</td>\n",
              "      <td>9866</td>\n",
              "      <td>89</td>\n",
              "      <td>83</td>\n",
              "      <td>0.011048</td>\n",
              "      <td>0.0125684</td>\n",
              "      <td>0.00902088</td>\n",
              "      <td>0.00841273</td>\n",
              "      <td>85</td>\n",
              "      <td>87</td>\n",
              "      <td>-0.0116279</td>\n",
              "      <td>13</td>\n",
              "      <td>0.388056</td>\n",
              "      <td>5.35522</td>\n",
              "      <td>2924</td>\n",
              "      <td>7535</td>\n",
              "      <td>50</td>\n",
              "      <td>59</td>\n",
              "      <td>0.0112807</td>\n",
              "      <td>0.0115461</td>\n",
              "      <td>0.0066357</td>\n",
              "      <td>0.00783013</td>\n",
              "      <td>134</td>\n",
              "      <td>185</td>\n",
              "      <td>-0.159875</td>\n",
              "      <td>13</td>\n",
              "      <td>0.393868</td>\n",
              "      <td>5.35755</td>\n",
              "      <td>4650</td>\n",
              "      <td>11806</td>\n",
              "      <td>117</td>\n",
              "      <td>106</td>\n",
              "      <td>0.0113502</td>\n",
              "      <td>0.01567</td>\n",
              "      <td>0.00991022</td>\n",
              "      <td>0.00897849</td>\n",
              "      <td>129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>0000012239</td>\n",
              "      <td>SPHERIX INC</td>\n",
              "      <td>200708</td>\n",
              "      <td>2007-08-14 00:00:00</td>\n",
              "      <td>10-Q</td>\n",
              "      <td>edgar/data/12239/0001104659-07-062470.txt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>152 rows × 49 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            CIK  ... constraining_words_whole_report\n",
              "0    0000003662  ...                            1452\n",
              "1    0000003662  ...                             NaN\n",
              "2    0000003662  ...                               5\n",
              "3    0000003662  ...                             691\n",
              "4    0000003662  ...                               4\n",
              "..          ...  ...                             ...\n",
              "147  0000012239  ...                             130\n",
              "148  0000012239  ...                               0\n",
              "149  0000012239  ...                              21\n",
              "150  0000012239  ...                             129\n",
              "151  0000012239  ...                             NaN\n",
              "\n",
              "[152 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffnb3HQQOdPb"
      },
      "source": [
        "file_rec.to_csv('/content/Final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMd_UQPHOtnH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}